{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRICARE:\n",
    "\n",
    "BRICARE consists of 2 different types of files by year:\n",
    "\n",
    "a. File after 2022 (2023-2024) = 79 kolom\n",
    "\n",
    "\n",
    "b. File before 2022 (2019-2022) = 27 kolom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type A\n",
    "\n",
    "\n",
    "Data Extraction for File Type A must be 2 Files:\n",
    "\n",
    "\n",
    "A.1 Columns (without \"Details\")\n",
    "\n",
    "\n",
    "A.2 Details only \n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['Create_Date','TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\"\n",
    "\n",
    "- Columns shoud be mapped based on their Call_Type_ID:\n",
    "\n",
    "['Produk','Jenis_Produk','Jenis_Laporan']\n",
    "\n",
    "- PLEASE ADD CIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20230101_20230101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\1840199040.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('NULL', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\1840199040.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('None', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\1840199040.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_file(file_path):\n",
    "\n",
    "    data = []\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            date_index = next(i for i, part in enumerate(parts) if date_pattern.match(part))\n",
    "\n",
    "            ticket_id = parts[0] \n",
    "            call_type_id = parts[1]  \n",
    "            description = ';'.join(parts[2:date_index])  \n",
    "            create_date = parts[date_index]  \n",
    "\n",
    "      \n",
    "            data.append([ticket_id, call_type_id, description, create_date] + parts[date_index + 1:])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.txt\"\n",
    "\n",
    "df = parse_file(file_path)\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "   \n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\3559677339.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\3559677339.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bricare_csv20230101_20230101.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) > 78:\n",
    "                description = ';'.join(parts[2:-75])\n",
    "                new_parts = parts[:2] + [description] + parts[-75:]\n",
    "                data.append(new_parts)\n",
    "            else:\n",
    "                data.append(parts)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df = df.replace(['0', 0], '')\n",
    "\n",
    "    columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "    \n",
    "    df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "    return df\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.csv\"\n",
    "df = parse_csv(file_path)\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_csv{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleasing the master Call Type file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_24036\\2288781540.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "master_df_path = r\"C:\\Users\\maste\\Downloads\\bricare\\(REVISED) SLA-OLA_NewUserGrouping_Ringkasan Kirim ME Versi 1.6.csv\"\n",
    "df = pd.read_csv(master_df_path, sep=';')\n",
    "\n",
    "\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "df = df.dropna(how='all')\n",
    "df.iloc[:450]\n",
    "df.to_csv(\"master_calltype.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call type mapping for columns 'Produk', 'Jenis Produk', 'Jenis Laporan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_dataset_path = r\"D:\\dataquality\\bricare_csv20230101_20230101.csv\"\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "master_df_path = r\"D:\\dataquality\\master_calltype.csv\"\n",
    "master_df = pd.read_csv(master_df_path)\n",
    "\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    'Case Types': 'Call_Type_ID', \n",
    "    'Product': 'Produk', \n",
    "    'Sub Product': 'Jenis_Produk', \n",
    "    'Case Category': 'Jenis_Laporan'\n",
    "})\n",
    "\n",
    "\n",
    "user_df['Call_Type_ID'] = user_df['Call_Type_ID'].astype(str)\n",
    "master_df['Call_Type_ID'] = master_df['Call_Type_ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(user_df, master_df[['Call_Type_ID', 'Produk', 'Jenis_Produk', 'Jenis_Laporan']], on='Call_Type_ID', how='left')\n",
    "\n",
    "user_df['Produk'] = merged_df['Produk_y']\n",
    "user_df['Jenis_Produk'] = merged_df['Jenis_Produk_y']\n",
    "user_df['Jenis_Laporan'] = merged_df['Jenis_Laporan_y']\n",
    "\n",
    "user_df.to_csv(user_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:  \n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]  \n",
    "                current_entry.append(parts[3].strip())  \n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def remove_bom_and_strip(df):\n",
    "    return df.applymap(lambda x: x.replace('\\ufeff', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.txt\"\n",
    "processed_data = process_text_data(file_path)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "df_final=df_final.iloc[:10]\n",
    "df_final.iloc[:10].to_csv('details_20230101_20230101.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to details_csv20230101_20230101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_csv20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the file A.1 and file A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality\\bricare_csv20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality\\details_csv20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality\\bricare_csv20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type B\n",
    "\n",
    "\n",
    "Data Extraction for File Type B (27 columns)\n",
    "\n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['TanggalClosed', 'tanggalTransaksi','Create_Date']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\" if any\n",
    "\n",
    "- PLEASE ADD CIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_33500\\3452762791.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20200101_20200101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\BRICARE_25042024 masking.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "\n",
    "data['Column1'] = data['Column1'].astype(str)\n",
    "data_cleaned = data[data['Column1'].str.match(r'TTB\\d+')]\n",
    "\n",
    "data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n",
    "data_cleaned = data_cleaned[data_cleaned['Column2'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "data_cleaned['Column4'] = pd.to_datetime(data_cleaned['Column4'], errors='coerce')\n",
    "data_cleaned = data_cleaned.dropna(subset=['Column4'])\n",
    "\n",
    "\n",
    "data_to_drop = ['Column28', 'Column29', 'Column30', 'Column31', 'Column32']\n",
    "data_cleaned = data_cleaned.drop(columns=data_to_drop)\n",
    "\n",
    "\n",
    "if len(data_cleaned.columns) <= len(column_list):\n",
    "    data_cleaned.columns = column_list[:len(data_cleaned.columns)]\n",
    "\n",
    "\n",
    "data_cleaned.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "data_cleaned.fillna('', inplace=True)\n",
    "data_cleaned = data_cleaned.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    data_cleaned[column] = pd.to_datetime(data_cleaned[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    data_cleaned[column] = data_cleaned[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Just take 10 lines for an example\n",
    "data_cleaned = data_cleaned.iloc[:10]\n",
    "\n",
    "\n",
    "data_cleaned.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(data_cleaned['Create_Date']))\n",
    "enddate = pd.Timestamp(max(data_cleaned['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "data_cleaned.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "27\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Details', 'Create_Date',\n",
      "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
      "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
      "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
      "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
      "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
      "       'user_login_name', 'Jenis_Produk', 'Last_Modified_By', 'Merchant_ID',\n",
      "       'Modified_Date', 'NOTAS', 'Produk', 'SLA_Status', 'TID',\n",
      "       'tanggalAttachmentDone', 'Tgl_Assigned', 'Tgl_Eskalasi', 'AnalisaSkils',\n",
      "       'Attachment_', 'Bank_BRI', 'Biaya_Admin', 'Suku_Bunga', 'Bunga',\n",
      "       'Butuh_Attachment', 'Cicilan', 'Hasil_Kunjungan', 'Log_Name',\n",
      "       'MMS_Ticket_Id', 'Mass_Ticket_Upload_Flag', 'Nama_Supervisor',\n",
      "       'Nama_TL', 'Nama_Wakabag', 'Nasabah_Prioritas', 'Notify_By',\n",
      "       'Organization', 'Output_Settlement', 'phone_survey', 'Return_Ticket',\n",
      "       'Settlement_By', 'Settlement_ID', 'Settlement', 'Site_User',\n",
      "       'Status_Return', 'Status_Transaksi', 'Submitter_Region',\n",
      "       'Submitter_SiteGroup', 'Submitter_User_group_ID', 'Tanggal_Settlement',\n",
      "       'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned', 'Ticket_Referensi',\n",
      "       'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users', 'Usergroup_ID'],\n",
      "      dtype='object')\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date', 'gateway',\n",
      "       'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal', 'status',\n",
      "       'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur', 'Nomor_Kartu',\n",
      "       'user_group', 'assgined_to', 'attachment_done', 'email', 'full_name',\n",
      "       'no_telepon', 'approver_login', 'approver_name', 'SLAResolution',\n",
      "       'submitter_login_id', 'submitter_user_group', 'user_login_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path1=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "path2=r\"D:\\dataquality\\bricare_20200101_20200101.csv\"\n",
    "\n",
    "df1=pd.read_csv(path1)\n",
    "df2=pd.read_csv(path2)\n",
    "print(len(df1.columns))\n",
    "print(len(df2.columns))\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Compare two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_match': True,\n",
       " 'columns_match': True,\n",
       " 'column_differences': {},\n",
       " 'value_differences': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly uploaded files for detailed comparison\n",
    "processed_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "expected_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "# Read the files\n",
    "processed_df_newest = pd.read_csv(processed_file_path_newest)\n",
    "expected_df_newest = pd.read_csv(expected_file_path_newest)\n",
    "\n",
    "# Check for exact match first\n",
    "exact_match = processed_df_newest.equals(expected_df_newest)\n",
    "\n",
    "# Initialize a dictionary to store detailed comparison results\n",
    "comparison_details = {\n",
    "    'shape_match': processed_df_newest.shape == expected_df_newest.shape,\n",
    "    'columns_match': processed_df_newest.columns.equals(expected_df_newest.columns),\n",
    "    'column_differences': {},\n",
    "    'value_differences': {}\n",
    "}\n",
    "\n",
    "# Compare each aspect if exact match is not true\n",
    "if not exact_match:\n",
    "    # Check shape\n",
    "    if not comparison_details['shape_match']:\n",
    "        comparison_details['shape_details'] = {\n",
    "            'processed_shape': processed_df_newest.shape,\n",
    "            'expected_shape': expected_df_newest.shape\n",
    "        }\n",
    "    \n",
    "    # Check columns\n",
    "    if not comparison_details['columns_match']:\n",
    "        comparison_details['column_details'] = {\n",
    "            'processed_columns': processed_df_newest.columns.tolist(),\n",
    "            'expected_columns': expected_df_newest.columns.tolist()\n",
    "        }\n",
    "\n",
    "    # Check column-by-column values\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            comparison_details['column_differences'][column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "\n",
    "# Summarize value differences\n",
    "if not comparison_details['columns_match']:\n",
    "    value_differences = {}\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            value_differences[column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "    comparison_details['value_differences'] = value_differences\n",
    "\n",
    "comparison_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To collect all Error logs in a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKET_ID</th>\n",
       "      <th>CALL_TYPE_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>DETAILS</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>GATEWAY</th>\n",
       "      <th>JENIS_LAPORAN</th>\n",
       "      <th>NAMA_NASABAH</th>\n",
       "      <th>NO_REKENING</th>\n",
       "      <th>NOMINAL</th>\n",
       "      <th>...</th>\n",
       "      <th>TGL_FOWARD</th>\n",
       "      <th>TGL_IN_PROGRESS</th>\n",
       "      <th>TGL_RETURNED</th>\n",
       "      <th>TICKET_REFERENSI</th>\n",
       "      <th>TIKET_URGENCY</th>\n",
       "      <th>TIPE_REMARK</th>\n",
       "      <th>UNIQUEID</th>\n",
       "      <th>USERS</th>\n",
       "      <th>USERGROUP_ID</th>\n",
       "      <th>ERROR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000043833835</td>\n",
       "      <td>8701</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>2023-01-01 07:07:15</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>Anwar</td>\n",
       "      <td>123456789012345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "      <td>Account ID: id value of incorrect type: TTB000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000043833951</td>\n",
       "      <td>8202</td>\n",
       "      <td>Informasi Product Banking</td>\n",
       "      <td>#BRILINKMOB\\n\\nDATA outlet BRILINK\\nKode Outle...</td>\n",
       "      <td>2023-01-01 07:06:39</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Products / Promotion Inquiry</td>\n",
       "      <td>Budi</td>\n",
       "      <td>123456789012345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "      <td>Account ID: id value of incorrect type: TTB000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000043833734</td>\n",
       "      <td>8701</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>#CALL TERPUTUS\\n\\nif ch call back ,layanan IB ...</td>\n",
       "      <td>2023-01-01 07:05:16</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>Ceshy</td>\n",
       "      <td>123456789012345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "      <td>Account ID: id value of incorrect type: TTB000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000043833965</td>\n",
       "      <td>8413</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>Nasabah gagal melakukan transaksi tarik tunai ...</td>\n",
       "      <td>2023-01-01 07:04:46</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>Dion Fury</td>\n",
       "      <td>123456789012345</td>\n",
       "      <td>2500000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>1</td>\n",
       "      <td>Account ID: id value of incorrect type: TTB000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TICKET_ID  CALL_TYPE_ID  \\\n",
       "0  TTB000043833835          8701   \n",
       "1  TTB000043833951          8202   \n",
       "2  TTB000043833734          8701   \n",
       "3  TTB000043833965          8413   \n",
       "\n",
       "                                           CALL_TYPE  \\\n",
       "0               Blokir Kartu ATM karena kartu hilang   \n",
       "1                          Informasi Product Banking   \n",
       "2               Blokir Kartu ATM karena kartu hilang   \n",
       "3  Nasabah BRI gagal tarik tunai & terdebet di AT...   \n",
       "\n",
       "                                             DETAILS          CREATE_DATE  \\\n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  2023-01-01 07:07:15   \n",
       "1  #BRILINKMOB\\n\\nDATA outlet BRILINK\\nKode Outle...  2023-01-01 07:06:39   \n",
       "2  #CALL TERPUTUS\\n\\nif ch call back ,layanan IB ...  2023-01-01 07:05:16   \n",
       "3  Nasabah gagal melakukan transaksi tarik tunai ...  2023-01-01 07:04:46   \n",
       "\n",
       "  GATEWAY                 JENIS_LAPORAN NAMA_NASABAH      NO_REKENING  \\\n",
       "0   Phone                       Request        Anwar  123456789012345   \n",
       "1   Phone  Products / Promotion Inquiry         Budi  123456789012345   \n",
       "2   Phone                       Request        Ceshy  123456789012345   \n",
       "3   Phone       Complaint - Transaction    Dion Fury  123456789012345   \n",
       "\n",
       "     NOMINAL  ...  TGL_FOWARD  TGL_IN_PROGRESS  TGL_RETURNED TICKET_REFERENSI  \\\n",
       "0        0.0  ...         NaN              NaN           NaN              NaN   \n",
       "1        0.0  ...         NaN              NaN           NaN              NaN   \n",
       "2        0.0  ...         NaN              NaN           NaN              NaN   \n",
       "3  2500000.0  ...         NaN              NaN           NaN              NaN   \n",
       "\n",
       "  TIKET_URGENCY  TIPE_REMARK UNIQUEID  USERS  USERGROUP_ID  \\\n",
       "0           NaN        Notes      NaN   Call             4   \n",
       "1           NaN        Notes      NaN   Call             4   \n",
       "2           NaN        Notes      NaN   Call             4   \n",
       "3           NaN        Notes      NaN   Call             1   \n",
       "\n",
       "                                               ERROR  \n",
       "0  Account ID: id value of incorrect type: TTB000...  \n",
       "1  Account ID: id value of incorrect type: TTB000...  \n",
       "2  Account ID: id value of incorrect type: TTB000...  \n",
       "3  Account ID: id value of incorrect type: TTB000...  \n",
       "\n",
       "[4 rows x 80 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[:4]\n",
    "# df.to_csv(\"bricare_test20230101_20230101.csv\", index=False)\n",
    "path2= r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\"\n",
    "df2=pd.read_csv(path2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined error logs saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "   \n",
    "\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Combined error logs saved to: {output_path}\")\n",
    "\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "combine_error_logs(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\maste\\\\Downloads\\\\dataloader_v60.0.2\\\\log\\\\error_logs.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Normalize the 'TICKET_ID' to ensure duplicates are identified\n",
    "    combined_df['TICKET_ID'] = combined_df['TICKET_ID'].str.upper()\n",
    "\n",
    "    # Remove duplicates based on the normalized 'TICKET_ID' column\n",
    "    combined_df = combined_df.drop_duplicates(subset='TICKET_ID')\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return output_path, combined_df\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "output_path, combined_df = combine_error_logs(directory_path)\n",
    "\n",
    "output_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
